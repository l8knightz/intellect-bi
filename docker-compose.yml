
version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ib-ollama
    environment:
      - OLLAMA_NUM_PARALLEL=1     # queue requests internally
      - OLLAMA_KEEP_ALIVE=10m     # keep model loaded between calls
      - OLLAMA_LOAD_TIMEOUT=10m   # allow time to load big models
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama:/root/.ollama

  api:
    hostname: api # had to add, api calls were not resolving to the container without it
    build:
      context: ./api
    container_name: ib-api
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - CHROMA_PERSIST_DIR=/app/vector
      - DATA_DIR=/app/data
      - ARTIFACTS_DIR=/app/artifacts
      - API_PORT=${API_PORT:-8000}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
      - CHAT_MODEL=${CHAT_MODEL:-phi3:mini}
    volumes:
      - ./data:/app/data
      - ./vector:/app/vector
      - ./artifacts:/app/artifacts
    depends_on:
      - ollama
    ports:
      - "${API_PORT:-8000}:8000"

  ui:
    build:
      context: ./ui
    container_name: ib-ui
    restart: unless-stopped
    environment:
      - API_URL=http://api:8000
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    depends_on:
      - api
    ports:
      - "${UI_PORT:-8501}:8501"
    volumes:
      - ./artifacts:/app/artifacts:ro
      - ./data:/app/data:ro

volumes:
  ollama:
